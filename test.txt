========課題１========

機械学習に関する基礎知識

工学部 電子情報学科
学籍番号：08D23091
辻 孝弥

1. Accuracy, Precision, Recall, F1-score の計算式

混同行列（Confusion Matrix）の要素：
・TP (True Positive): 真陽性 - 正しく正と予測された数
・TN (True Negative): 真陰性 - 正しく負と予測された数
・FP (False Positive): 偽陽性 - 誤って正と予測された数
・FN (False Negative): 偽陰性 - 誤って負と予測された数

Accuracy（正解率）
計算式: (TP + TN) / (TP + TN + FP + FN)
説明：全体のデータの中で、正しく予測できた割合を表す。

Precision（適合率）
計算式: TP / (TP + FP)
説明：正と予測したデータの中で、実際に正であった割合を表す。

Recall（再現率）
計算式: TP / (TP + FN)
説明：実際に正であるデータの中で、正と予測できた割合を表す。

F1-score（F1値）
計算式: 2 * (Precision * Recall) / (Precision + Recall)
説明：Precision と Recall の調和平均である。両者のバランスを考慮した指標で、どちらか一方だけが極端に高い場合に低い値になる。クラスの偏りが大きいデータセットでAccuracyよりも有用である。

2. deepcopy と copy 関数の違い

・copy: オブジェクトの参照をコピーする。元のオブジェクトとコピーされたオブジェクトは、同じデータを参照するため、一方を変更するともう一方も変更される。

・deepcopy: オブジェクトとその内部のオブジェクトも含めて、すべてを新たにコピーする。元のオブジェクトとコピーされたオブジェクトは完全に独立しており、一方を変更してももう一方に影響はない。

3. optimizer.zero_grad() の有無による動作の違い

optimizer.zero_grad() は、モデルのパラメータの勾配を0にリセットする関数である。

optimizer.zero_grad() がある場合：
各 iteration（バッチ）の前に勾配が0にリセットされるため、勾配が累積されずに、現在のバッチのデータに基づいてパラメータが更新される。

optimizer.zero_grad() がない場合：
・勾配が iteration 毎に累積されていく。
・過去の勾配情報が現在の更新に影響を与えるため、学習が不安定になったり、意図した方向に学習が進まなかったりする可能性がある。

影響：
optimizer.zero_grad() を実行しないと、以下のような問題が生じる可能性がある：
・loss が発散しやすくなる
・モデルの学習速度が遅くなる
・モデルが収束しなくなる

結論：
optimizer.zero_grad() は、各 iteration の前に勾配をリセットすることで、安定した学習を保証するために必要な処理である。 

参考文献
・Google Developers: Machine Learning Crash Course - Classification: Accuracy, Precision, Recall
  https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall?hl=ja









========課題３========

COMPASデータセットを用いた再犯予測モデルの精度向上

工学部 電子情報学科
学籍番号：08D23091
辻 孝弥

1. はじめに
本課題では，COMPASデータセットを用いて再犯予測モデルの精度向上を目的とした。XGBoostとMLPの２種類のモデルを構築・改善し，さらに両者をスタッキングする手法を適用した。

2. 実験方法
2.1 XGBoostモデルの導入とハイパーパラメータ調整
使用モデル: XGBClassifier

2.1.1 Optunaによる事前探索と二段階学習
Optuna を用いて最適なハイパーパラメータを事前に調査し，その結果を best_params に設定して一次学習を実施した。
続いて一次学習済みモデルで派生特徴量を生成し，クラス不均衡補正などを加えた新たなパラメータセット new_params で再度全データを用いた二次学習を行った。

2.1.2 主なハイパーパラメータ設定
・学習率 (learning_rate)：0.05 → 0.08
・木の深さ (max_depth)：3
・サブサンプリング率 (subsample)：0.90
・特徴量サンプリング率 (colsample_bytree)：0.78
・最小子ノード重量 (min_child_weight)：5
・正則化パラメータ (gamma)：0.50
・L1正則化 (reg_alpha)：5.4×10^(-6)
・L2正則化 (reg_lambda)：5.88
・木数 (n_estimators)：200
・早期停止 (early_stopping_rounds)：10
・クラス不均衡補正 (scale_pos_weight)：訓練データのクラス比に基づき自動設定

2.2 特徴量エンジニアリング
2.2.1 不要特徴量の削減
重要度上位80%を残し，下位20%を削除（priors_count・age は保護）

2.2.2 ChatGPTによる特徴量選択
全てのCSVカラムを逐次試すのは非効率なため，ChatGPT を用いて平均的に効果が見込める無難な派生特徴量案を取得し，それを見ながら実装した。

2.2.3 主な派生特徴量
・priors_per_year（前科数／(年齢＋1)）
・sum_priors_and_age（前科数＋年齢）
・age_squared（年齢^2）
・log_priors_p1（log(前科数＋1)）
・age_times_priors（年齢×前科数）
・total_juv_cnt（少年期犯罪合計）
・juv_ratio（少年期犯罪合計／(前科数＋1)）
・log_len_stay（拘束期間の対数化）

2.2.4 dob（生年月日）の扱い
・初期には再犯率と無関係と判断し除外したが，除外時のAccuracyが0.688→0.679に低下
・最終的には dob を含める実装とし，Accuracyを0.699→0.710へ改善

2.3 MLPモデルの構造・学習戦略改善
2.3.1 ネットワーク構造
隠れ層：256→128，ReLU＋BatchNorm＋Dropout(0.3/0.2)

2.3.2 学習戦略
・Optimizer：Adam(lr=1e-3, weight_decay=1e-5)
・Scheduler：CosineAnnealingLR
・損失関数：クラス重み付きCrossEntropy
・EarlyStopping：patience=15

2.4 モデルスタッキング
・メタ学習器：LogisticRegression (L2, C=1.0)
・入力特徴：XGBoost/MLP の検証データ予測確率
・閾値決定：Youden's J による最適閾値

3. 結果
モデル性能比較：
①MLP 単体: 0.675
②XGBoost 単体＋特徴量変更: 0.688
③②モデル（dob除外）: 0.679
④今回実装モデル（dob除外）: 0.699
⑤完全実装モデル（dob含む）: 0.710

⑤の詳細ログ
混同行列：
[[1591  421]
 [ 627  968]]

分類レポート：
              precision    recall  f1-score   support
           0      0.717     0.791     0.752      2012
           1      0.697     0.607     0.649      1595

    accuracy                          0.710      3607
   macro avg      0.707     0.699     0.701      3607
weighted avg      0.708     0.710     0.706      3607

Accuracy: 0.710
ROC-AUC : 0.767
LogLoss : 0.576

4. 考察
4.1 Optunaによるハイパラ最適化
一次学習で得られたパラメータをもとに，二次学習時にはクラス不均衡補正などを加えた new_params を適用し，性能向上に寄与した。

4.2 ChatGPT活用の特徴量設計
無難で効果の期待できる特徴量案を迅速に取得でき，実装工数を大幅に削減できた。

4.3 dob除外の効果検証
当初「再犯率に関係がない」と判断して dob を除外したが，Accuracyが0.688→0.679に低下した。
dob を含めることで0.699→0.710へ改善し，生年月日情報が有用であることを確認した。

4.4 スタッキング効果
異なるモデルの補完性により，最終的にAccuracy:0.710／ROC–AUC:0.767／LogLoss:0.576を達成した。

5. 結論
Optunaによる二段階ハイパラ最適化とChatGPT提案の特徴量エンジニアリングを組み合わせ，実装に忠実に dob 情報の有効性を再検証した結果，再犯予測タスクにおいて高い汎化性能を実現できた。今後はさらに異なる情報源やモデル統合手法を探索し，性能向上を図る余地がある。

参考文献
・スタッキングの実装と効果について
  https://potesara-tips.com/ensemble-stacking/#toc13 